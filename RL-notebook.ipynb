{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ticker symbol for DJIA\n",
    "ticker = \"^DJI\"\n",
    "\n",
    "# Download historical data from Yahoo Finance\n",
    "df = yf.download(ticker, start=\"2000-01-01\", end=\"2024-01-01\")\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the data is sorted by date\n",
    "df_time = df.sort_index()\n",
    "\n",
    "# Perform time series decomposition on the open price\n",
    "open_price = df_time['Open']\n",
    "result = seasonal_decompose(open_price, model='additive', period=252)  # Assuming 252 trading days in a year\n",
    "\n",
    "# Plot the decomposition\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(15, 12))\n",
    "\n",
    "# Observed\n",
    "result.observed.plot(ax=ax1)\n",
    "ax1.set_ylabel('Observed')\n",
    "ax1.set_title('Observed Open Prices')\n",
    "\n",
    "# Trend\n",
    "result.trend.plot(ax=ax2)\n",
    "ax2.set_ylabel('Trend')\n",
    "ax2.set_title('Trend Component')\n",
    "\n",
    "# Seasonal\n",
    "result.seasonal.plot(ax=ax3)\n",
    "ax3.set_ylabel('Seasonal')\n",
    "ax3.set_title('Seasonal Component')\n",
    "\n",
    "# Residual\n",
    "result.resid.plot(ax=ax4)\n",
    "ax4.set_ylabel('Residual')\n",
    "ax4.set_title('Residual Component')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Moving Averages\n",
    "df['SMA_20'] = df['Close'].rolling(window=20).mean()  # 20-day Simple Moving Average\n",
    "df['SMA_50'] = df['Close'].rolling(window=50).mean()  # 50-day Simple Moving Average\n",
    "df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean()  # 12-day Exponential Moving Average\n",
    "df['EMA_26'] = df['Close'].ewm(span=26, adjust=False).mean()  # 26-day Exponential Moving Average\n",
    "\n",
    "# Calculate RSI\n",
    "delta = df['Close'].diff(1)\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "\n",
    "rs = avg_gain / avg_loss\n",
    "df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Calculate MACD\n",
    "df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# Handle missing values by filling forward\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Display the updated DataFrame with new features\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame with the same column names\n",
    "scaled_df = pd.DataFrame(scaled_data, index=df.index, columns=df.columns)\n",
    "\n",
    "# Display the first few rows of the scaled data\n",
    "print(scaled_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, df):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.current_step = 0\n",
    "        self.initial_balance = 10000\n",
    "        self.balance = self.initial_balance\n",
    "        self.asset = 0\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.transaction_cost = 0.001  # Example transaction cost (0.1%)\n",
    "        self.max_position_size = 100000  # Limit on position size\n",
    "\n",
    "        # Define the action space (Hold, Buy, Sell)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Observation space: balance, asset, net worth, SMA, RSI, MACD\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
    "\n",
    "        # History tracking\n",
    "        self.balance_history = []\n",
    "        self.asset_history = []\n",
    "        self.net_worth_history = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.asset = 0\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.balance_history = [self.balance]\n",
    "        self.asset_history = [self.asset]\n",
    "        self.net_worth_history = [self.net_worth]\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = np.array([\n",
    "            self.balance, \n",
    "            self.asset, \n",
    "            self.net_worth,\n",
    "            self.df.iloc[self.current_step]['SMA_20'],\n",
    "            self.df.iloc[self.current_step]['RSI'],\n",
    "            self.df.iloc[self.current_step]['MACD']\n",
    "        ])\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.df.iloc[self.current_step]['Close']\n",
    "        self.current_step += 1\n",
    "\n",
    "        prev_net_worth = self.net_worth\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            if self.balance > 0:\n",
    "                amount_to_buy = min(self.balance, self.max_position_size)\n",
    "                self.asset += (amount_to_buy * (1 - self.transaction_cost)) / current_price\n",
    "                self.balance -= amount_to_buy\n",
    "        elif action == 2:  # Sell\n",
    "            if self.asset > 0:\n",
    "                self.balance += self.asset * current_price * (1 - self.transaction_cost)\n",
    "                self.asset = 0\n",
    "\n",
    "        self.net_worth = self.balance + self.asset * current_price\n",
    "\n",
    "        # Ensure balance doesn't go negative\n",
    "        if self.balance < 0:\n",
    "            self.balance = 0\n",
    "            self.net_worth = self.asset * current_price\n",
    "\n",
    "        # Ensure net worth doesn't go negative\n",
    "        if self.net_worth < 0:\n",
    "            self.net_worth = prev_net_worth\n",
    "\n",
    "        # Record history\n",
    "        self.balance_history.append(self.balance)\n",
    "        self.asset_history.append(self.asset)\n",
    "        self.net_worth_history.append(self.net_worth)\n",
    "\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f'Step: {self.current_step}')\n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(f'Asset: {self.asset}')\n",
    "        print(f'Net Worth: {self.net_worth}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading Agent(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_bins, action_size):\n",
    "        self.state_bins = state_bins\n",
    "        self.action_size = action_size\n",
    "        self.q_table = np.zeros(tuple(len(bins) + 1 for bins in state_bins) + (action_size,))\n",
    "        self.learning_rate = 0.1\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        state_indices = []\n",
    "        for i, value in enumerate(state):\n",
    "            state_index = np.digitize(value, self.state_bins[i]) - 1\n",
    "            state_indices.append(state_index)\n",
    "        return tuple(state_indices)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        return np.argmax(self.q_table[state])\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.gamma * self.q_table[next_state][best_next_action] * (1 - done)\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "\n",
    "        if done:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Define the DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=1000)  # Reduce memory size\n",
    "        self.gamma = 0.95    # Discount rate\n",
    "        self.epsilon = 1.0   # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 12),  # Reduce network complexity\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, self.action_size)\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state)\n",
    "        act_values = self.model(state)\n",
    "        return np.argmax(act_values.detach().numpy())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state = torch.FloatTensor(next_state)\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n",
    "\n",
    "            state = torch.FloatTensor(state)\n",
    "            current_q_values = self.model(state)\n",
    "            target_q_values = current_q_values.clone().detach()\n",
    "            target_q_values[action] = target\n",
    "\n",
    "            # Perform the backward pass and update the weights\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Trading Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = TradingEnv(scaled_df)\n",
    "\n",
    "# Reset the environment\n",
    "state = env.reset()\n",
    "\n",
    "# Take a sample step\n",
    "action = env.action_space.sample()  # Random action (for illustration)\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "# Render the environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Q-learning Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Environment and Agent for Q-Learning\n",
    "env_qlearning = TradingEnv(scaled_df)\n",
    "\n",
    "state_bins = [\n",
    "    np.linspace(0, env_qlearning.initial_balance * 2, 20),  # Balance bins\n",
    "    np.linspace(0, env_qlearning.initial_balance * 2, 20),  # Asset bins\n",
    "    np.linspace(0, env_qlearning.initial_balance * 2, 20),  # Net worth bins\n",
    "    np.linspace(-5, 5, 20),                                 # SMA_20 bins\n",
    "    np.linspace(0, 100, 20),                                # RSI bins\n",
    "    np.linspace(-5, 5, 20)                                  # MACD bins\n",
    "]\n",
    "\n",
    "agent_qlearning = QLearningAgent(state_bins, env_qlearning.action_space.n)\n",
    "\n",
    "# Train the Q-learning Agent\n",
    "num_episodes_qlearning = 100\n",
    "\n",
    "for episode in range(num_episodes_qlearning):\n",
    "    state = agent_qlearning.discretize_state(env_qlearning.reset())\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent_qlearning.choose_action(state)\n",
    "        next_state, reward, done, _ = env_qlearning.step(action)\n",
    "        next_state = agent_qlearning.discretize_state(next_state)\n",
    "        agent_qlearning.learn(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Q-Learning Episode {episode + 1}: Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the DQN Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Environment and Agent for DQN\n",
    "env_dqn = TradingEnv(scaled_df)\n",
    "state_size = env_dqn.observation_space.shape[0]\n",
    "action_size = env_dqn.action_space.n\n",
    "agent_dqn = DQNAgent(state_size, action_size)\n",
    "batch_size = 16  # Reduce batch size\n",
    "num_episodes_dqn = 50  # Reduce number of episodes\n",
    "\n",
    "# Train the DQN Agent\n",
    "for e in range(num_episodes_dqn):\n",
    "    state = env_dqn.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for time in range(200):  # Reduce the number of steps per episode\n",
    "        action = agent_dqn.act(state)\n",
    "        next_state, reward, done, _ = env_dqn.step(action)\n",
    "        total_reward += reward\n",
    "        agent_dqn.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(f\"DQN Episode {e + 1}/{num_episodes_dqn}, Reward: {total_reward}, Epsilon: {agent_dqn.epsilon}\")\n",
    "            break\n",
    "\n",
    "        if len(agent_dqn.memory) > batch_size:\n",
    "            agent_dqn.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics Calculation\n",
    "def calculate_metrics(balance_history, initial_balance=10000):\n",
    "    balance_series = pd.Series(balance_history)\n",
    "    returns = balance_series.pct_change().dropna()\n",
    "\n",
    "    # Sharpe Ratio\n",
    "    risk_free_rate = 0.0\n",
    "    average_return = returns.mean()\n",
    "    std_dev = returns.std()\n",
    "    sharpe_ratio = (average_return - risk_free_rate) / std_dev if std_dev != 0 else 0\n",
    "\n",
    "    # Cumulative Returns\n",
    "    cumulative_returns = (balance_series.iloc[-1] / initial_balance) - 1\n",
    "\n",
    "    # Maximum Drawdown\n",
    "    rolling_max = balance_series.cummax()\n",
    "    drawdown = (balance_series - rolling_max) / rolling_max\n",
    "    max_drawdown = drawdown.min()\n",
    "\n",
    "    return {\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Cumulative Returns': cumulative_returns,\n",
    "        'Maximum Drawdown': max_drawdown\n",
    "    }\n",
    "\n",
    "# Plotting Performance\n",
    "def plot_performance(balance_history, asset_history, net_worth_history):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(balance_history, label='Balance')\n",
    "    plt.plot(asset_history, label='Assets')\n",
    "    plt.plot(net_worth_history, label='Net Worth')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Trading Bot Performance')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the RL Trading Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Qlearning Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Q-learning Agent in the Environment\n",
    "state = agent_qlearning.discretize_state(env_qlearning.reset())\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = agent_qlearning.choose_action(state)\n",
    "    next_state, reward, done, _ = env_qlearning.step(action)\n",
    "    next_state = agent_qlearning.discretize_state(next_state)\n",
    "    state = next_state\n",
    "\n",
    "env_qlearning.render()\n",
    "qlearning_metrics = calculate_metrics(env_qlearning.net_worth_history)\n",
    "print(f\"Q-Learning Performance: {qlearning_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run DQN Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the DQN Agent in the Environment\n",
    "state = env_dqn.reset()\n",
    "done = False\n",
    "dqn_rewards = []\n",
    "\n",
    "while not done:\n",
    "    action = agent_dqn.act(state)\n",
    "    next_state, reward, done, _ = env_dqn.step(action)\n",
    "    state = next_state\n",
    "    dqn_rewards.append(reward)\n",
    "\n",
    "env_dqn.render()\n",
    "print(f\"DQN Total Reward: {np.sum(dqn_rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance for Q-Learning\n",
    "plot_performance(env_qlearning.balance_history, env_qlearning.asset_history, env_qlearning.net_worth_history)\n",
    "\n",
    "# Plot performance for DQN\n",
    "plot_performance(env_dqn.balance_history, env_dqn.asset_history, env_dqn.net_worth_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
